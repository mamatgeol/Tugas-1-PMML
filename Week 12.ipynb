{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e782ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import gspread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f89235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://books.toscrape.com/'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c524571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/page-1.html\n",
      "https://books.toscrape.com/catalogue/page-2.html\n",
      "https://books.toscrape.com/catalogue/page-3.html\n",
      "https://books.toscrape.com/catalogue/page-4.html\n",
      "https://books.toscrape.com/catalogue/page-5.html\n",
      "https://books.toscrape.com/catalogue/page-6.html\n",
      "https://books.toscrape.com/catalogue/page-7.html\n",
      "https://books.toscrape.com/catalogue/page-8.html\n",
      "https://books.toscrape.com/catalogue/page-9.html\n",
      "https://books.toscrape.com/catalogue/page-10.html\n",
      "https://books.toscrape.com/catalogue/page-11.html\n",
      "https://books.toscrape.com/catalogue/page-12.html\n",
      "https://books.toscrape.com/catalogue/page-13.html\n",
      "https://books.toscrape.com/catalogue/page-14.html\n",
      "https://books.toscrape.com/catalogue/page-15.html\n",
      "https://books.toscrape.com/catalogue/page-16.html\n",
      "https://books.toscrape.com/catalogue/page-17.html\n",
      "https://books.toscrape.com/catalogue/page-18.html\n",
      "https://books.toscrape.com/catalogue/page-19.html\n",
      "https://books.toscrape.com/catalogue/page-20.html\n",
      "https://books.toscrape.com/catalogue/page-21.html\n",
      "https://books.toscrape.com/catalogue/page-22.html\n",
      "https://books.toscrape.com/catalogue/page-23.html\n",
      "https://books.toscrape.com/catalogue/page-24.html\n",
      "https://books.toscrape.com/catalogue/page-25.html\n",
      "https://books.toscrape.com/catalogue/page-26.html\n",
      "https://books.toscrape.com/catalogue/page-27.html\n",
      "https://books.toscrape.com/catalogue/page-28.html\n",
      "https://books.toscrape.com/catalogue/page-29.html\n",
      "https://books.toscrape.com/catalogue/page-30.html\n",
      "https://books.toscrape.com/catalogue/page-31.html\n",
      "https://books.toscrape.com/catalogue/page-32.html\n",
      "https://books.toscrape.com/catalogue/page-33.html\n",
      "https://books.toscrape.com/catalogue/page-34.html\n",
      "https://books.toscrape.com/catalogue/page-35.html\n",
      "https://books.toscrape.com/catalogue/page-36.html\n",
      "https://books.toscrape.com/catalogue/page-37.html\n",
      "https://books.toscrape.com/catalogue/page-38.html\n",
      "https://books.toscrape.com/catalogue/page-39.html\n",
      "https://books.toscrape.com/catalogue/page-40.html\n",
      "https://books.toscrape.com/catalogue/page-41.html\n",
      "https://books.toscrape.com/catalogue/page-42.html\n",
      "https://books.toscrape.com/catalogue/page-43.html\n",
      "https://books.toscrape.com/catalogue/page-44.html\n",
      "https://books.toscrape.com/catalogue/page-45.html\n",
      "https://books.toscrape.com/catalogue/page-46.html\n",
      "https://books.toscrape.com/catalogue/page-47.html\n",
      "https://books.toscrape.com/catalogue/page-48.html\n",
      "https://books.toscrape.com/catalogue/page-49.html\n",
      "https://books.toscrape.com/catalogue/page-50.html\n",
      "Found 50 pages.\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "i=1\n",
    "while True:\n",
    "  url_page = f\"{url}catalogue/page-{i}.html\"\n",
    "  page_response = requests.get(url_page)\n",
    "  if page_response.status_code == 200:\n",
    "    pages.append(url_page)\n",
    "    print(url_page)\n",
    "    i+=1\n",
    "  else:\n",
    "    break\n",
    "print(f\"Found {len(pages)} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0562cbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       " 'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       " 'https://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       " 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       " 'https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-requiem-red_995/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-black-maria_991/index.html',\n",
       " 'https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',\n",
       " 'https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',\n",
       " 'https://books.toscrape.com/catalogue/set-me-free_988/index.html',\n",
       " 'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       " 'https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',\n",
       " 'https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',\n",
       " 'https://books.toscrape.com/catalogue/olio_984/index.html',\n",
       " 'https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',\n",
       " 'https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',\n",
       " 'https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page1 = 'https://books.toscrape.com/catalogue/page-1.html'\n",
    "page_response=requests.get(page1)\n",
    "page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "books = [url + 'catalogue/' + link.get('href')for link in page_soup.select('h3 a')]\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c3db95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/page-1.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-2.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-3.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-4.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-5.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-6.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-7.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-8.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-9.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-10.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-11.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-12.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-13.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-14.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-15.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-16.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-17.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-18.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-19.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-20.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-21.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-22.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-23.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-24.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-25.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-26.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-27.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-28.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-29.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-30.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-31.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-32.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-33.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-34.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-35.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-36.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-37.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-38.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-39.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-40.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-41.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-42.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-43.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-44.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-45.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-46.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-47.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-48.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-49.html : 20books.\n",
      "https://books.toscrape.com/catalogue/page-50.html : 20books.\n",
      "Found 1000 books.\n"
     ]
    }
   ],
   "source": [
    "book_urls = []\n",
    "for page in pages:\n",
    "  page_response=requests.get(page)\n",
    "  page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "  book_links = [url + 'catalogue/'+ link.get('href')for link in page_soup.select('h3 a')]\n",
    "  book_urls.extend(book_links)\n",
    "  print(f\"{page} : {len(book_links)}books.\")\n",
    "\n",
    "print(f\"Found {len(book_urls)} books.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a9e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poetry\n",
      "A Light in the Attic\n",
      "Three\n",
      "It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\n",
      "https://books.toscrape.com/media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(cover)\n\u001b[32m     22\u001b[39m product_info = book_soup.select_one(\u001b[33m'\u001b[39m\u001b[33mtable.table-striped tr\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m code = \u001b[43mproduct_info\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.select_one(\u001b[33m'\u001b[39m\u001b[33mtd\u001b[39m\u001b[33m'\u001b[39m).get_text()\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(code)\n\u001b[32m     27\u001b[39m price_incl_tax = \u001b[38;5;28mfloat\u001b[39m(product_info[\u001b[32m2\u001b[39m].select_one(\u001b[33m'\u001b[39m\u001b[33mtd\u001b[39m\u001b[33m'\u001b[39m).get_text().replace(\u001b[33m'\u001b[39m\u001b[33m£\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\bs4\\element.py:2401\u001b[39m, in \u001b[36mTag.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) -> _AttributeValue:\n\u001b[32m   2399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"tag[key] returns the value of the 'key' attribute for the Tag,\u001b[39;00m\n\u001b[32m   2400\u001b[39m \u001b[33;03m    and throws an exception if it's not there.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "book_url_1 = book_urls[0]\n",
    "book_response = requests.get(book_url_1)\n",
    "book_soup = BeautifulSoup(book_response.content, 'html.parser')\n",
    "\n",
    "category = book_soup.select_one('.breadcrumb li:nth-child(3) a').get_text()\n",
    "print(category)\n",
    "\n",
    "title = book_soup.select_one('h1').get_text()\n",
    "print(title)\n",
    "\n",
    "rating = book_soup.select_one('p.star-rating')['class'][1]\n",
    "print(rating)\n",
    "\n",
    "description_element = book_soup.select_one('#product_description ~ p')\n",
    "description = description_element.get_text() if description_element else 'No description available'\n",
    "print(description)\n",
    "\n",
    "cover = url + book_soup.select_one('#product_gallery img')['src'].replace('../','')\n",
    "print(cover)\n",
    "\n",
    "product_info = book_soup.select_one('table.table-striped tr')\n",
    "\n",
    "code = product_info[0].select_one('td').get_text()\n",
    "print(code)\n",
    "\n",
    "price_incl_tax = float(product_info[2].select_one('td').get_text().replace('£', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "scrape_books_toscrape.py\n",
    "Scrapes https://books.toscrape.com/ to produce a dataframe of ~1000 books with requested fields.\n",
    "\n",
    "Usage:\n",
    "    python scrape_books_toscrape.py\n",
    "\n",
    "Outputs:\n",
    "    - books.csv (CSV)\n",
    "    - books.parquet (optional, faster to load)\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "BASE_URL = \"https://books.toscrape.com/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; Scraper/1.0; +https://example.com/bot)\"\n",
    "}\n",
    "\n",
    "# map rating word in class to number\n",
    "RATING_MAP = {\n",
    "    \"One\": 1,\n",
    "    \"Two\": 2,\n",
    "    \"Three\": 3,\n",
    "    \"Four\": 4,\n",
    "    \"Five\": 5\n",
    "}\n",
    "\n",
    "\n",
    "def get_soup(url: str, session: requests.Session) -> BeautifulSoup:\n",
    "    r = session.get(url, headers=HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def parse_book_page(book_url: str, session: requests.Session) -> Dict:\n",
    "    \"\"\"Parse one product page and return dict with required fields.\"\"\"\n",
    "    soup = get_soup(book_url, session)\n",
    "\n",
    "    # TITLE\n",
    "    title_tag = soup.find(\"h1\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "    # CATEGORY: breadcrumb last li (active)\n",
    "    category = \"\"\n",
    "    breadcrumb = soup.select_one(\"ul.breadcrumb\")\n",
    "    if breadcrumb:\n",
    "        # last li with class 'active' usually holds category or product name (site uses category at 3rd pos)\n",
    "        # On product page the breadcrumb structure is Home > Books > Category > Title\n",
    "        lis = breadcrumb.find_all(\"li\")\n",
    "        if len(lis) >= 3:\n",
    "            # Category is the 3rd li (index 2) normally\n",
    "            cat_li = lis[-2]  # second-to-last is category\n",
    "            category = cat_li.get_text(strip=True)\n",
    "\n",
    "    # DESCRIPTION: product_description heading then next <p>\n",
    "    description = \"\"\n",
    "    prod_desc_h = soup.find(\"div\", id=\"product_description\")\n",
    "    if prod_desc_h:\n",
    "        desc_p = prod_desc_h.find_next_sibling(\"p\")\n",
    "        if desc_p:\n",
    "            description = desc_p.get_text(strip=True)\n",
    "\n",
    "    # COVER IMAGE\n",
    "    cover_img = \"\"\n",
    "    img_tag = soup.select_one(\"div.item.active img\") or soup.select_one(\"div.carousel-inner img\") or soup.select_one(\"div.product_gallery img\")\n",
    "    if not img_tag:\n",
    "        img_tag = soup.select_one(\"div.thumbnail img\")  # fallback\n",
    "    if img_tag and img_tag.get(\"src\"):\n",
    "        cover_src = img_tag[\"src\"]\n",
    "        cover_img = urljoin(book_url, cover_src)\n",
    "\n",
    "    # PRODUCT TABLE: UPC, prices, tax, availability, number of reviews\n",
    "    product_info = {}\n",
    "    table = soup.find(\"table\", class_=\"table table-striped\")\n",
    "    if table:\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            th = row.find(\"th\")\n",
    "            td = row.find(\"td\")\n",
    "            if not th or not td:\n",
    "                continue\n",
    "            key = th.get_text(strip=True)\n",
    "            val = td.get_text(strip=True)\n",
    "            product_info[key] = val\n",
    "\n",
    "    # fields from product_info\n",
    "    code = product_info.get(\"UPC\", \"\")\n",
    "    price_excl = product_info.get(\"Price (excl. tax)\", \"\")\n",
    "    price_incl = product_info.get(\"Price (incl. tax)\", \"\")\n",
    "    tax = product_info.get(\"Tax\", \"\")\n",
    "    number_of_reviews = product_info.get(\"Number of reviews\", \"\")\n",
    "    # normalize number_of_reviews to int if possible\n",
    "    try:\n",
    "        number_of_reviews = int(number_of_reviews)\n",
    "    except:\n",
    "        number_of_reviews = None\n",
    "\n",
    "    # STOCK STATUS and number available\n",
    "    availability_text = product_info.get(\"Availability\", \"\") or \"\"\n",
    "    stock_status = availability_text\n",
    "    number_in_stock = None\n",
    "    # example availability: \"In stock (22 available)\"\n",
    "    m = re.search(r\"\\((\\d+)\\s+available\\)\", availability_text)\n",
    "    if m:\n",
    "        number_in_stock = int(m.group(1))\n",
    "    else:\n",
    "        # sometimes only \"In stock\"\n",
    "        if \"In stock\" in availability_text:\n",
    "            number_in_stock = None  # unspecified\n",
    "        else:\n",
    "            number_in_stock = 0\n",
    "\n",
    "    # RATING: look for <p class=\"star-rating Three\">\n",
    "    rating_val = None\n",
    "    rating_p = soup.find(\"p\", class_=re.compile(r\"star-rating\"))\n",
    "    if rating_p:\n",
    "        classes = rating_p.get(\"class\", [])\n",
    "        # classes contain 'star-rating' and the word e.g. 'Three'\n",
    "        for c in classes:\n",
    "            if c in RATING_MAP:\n",
    "                rating_val = RATING_MAP[c]\n",
    "                break\n",
    "\n",
    "    result = {\n",
    "        \"category\": category,\n",
    "        \"code\": code,\n",
    "        \"cover\": cover_img,\n",
    "        \"title\": title,\n",
    "        \"rating\": rating_val,\n",
    "        \"price (excl. tax)\": price_excl,\n",
    "        \"price (incl. tax)\": price_incl,\n",
    "        \"tax\": tax,\n",
    "        \"stock status\": stock_status,\n",
    "        \"number of stock available\": number_in_stock,\n",
    "        \"description\": description,\n",
    "        \"number of reviews\": number_of_reviews,\n",
    "        \"product_page_url\": book_url\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_book_links_from_listing(page_soup: BeautifulSoup, base_page_url: str) -> List[str]:\n",
    "    \"\"\"Extract absolute product page URLs from a listing page soup.\"\"\"\n",
    "    links = []\n",
    "    # product_pod -> h3 -> a[href]\n",
    "    for article in page_soup.select(\"article.product_pod\"):\n",
    "        a = article.find(\"h3\").find(\"a\")\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            full = urljoin(base_page_url, href)\n",
    "            # site sometimes uses relative paths with '../', urljoin handles it\n",
    "            links.append(full)\n",
    "    return links\n",
    "\n",
    "\n",
    "def find_next_page(soup: BeautifulSoup, current_page_url: str) -> str:\n",
    "    \"\"\"Return absolute URL of 'next' page if exists else None.\"\"\"\n",
    "    li = soup.find(\"li\", class_=\"next\")\n",
    "    if li and li.find(\"a\"):\n",
    "        href = li.find(\"a\")[\"href\"]\n",
    "        return urljoin(current_page_url, href)\n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_all_books(max_books: int = 1100, delay_range=(0.2, 0.6)) -> pd.DataFrame:\n",
    "    \"\"\"Scrape until we collect max_books or no more pages. Default max slightly >1000.\"\"\"\n",
    "    session = requests.Session()\n",
    "    results = []\n",
    "    visited_books = set()\n",
    "\n",
    "    next_page = BASE_URL  # starts at homepage\n",
    "    page_count = 0\n",
    "\n",
    "    while next_page and len(results) < max_books:\n",
    "        page_count += 1\n",
    "        print(f\"[page {page_count}] fetching listing: {next_page}\")\n",
    "        try:\n",
    "            soup = get_soup(next_page, session)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch listing page {next_page}: {e}\")\n",
    "            break\n",
    "\n",
    "        book_links = get_book_links_from_listing(soup, next_page)\n",
    "        print(f\"  -> found {len(book_links)} product links on page\")\n",
    "\n",
    "        for link in book_links:\n",
    "            # Some listing links are to the same product across pages; skip duplicates\n",
    "            if link in visited_books:\n",
    "                continue\n",
    "            visited_books.add(link)\n",
    "\n",
    "            # polite delay\n",
    "            time.sleep(random.uniform(*delay_range))\n",
    "\n",
    "            try:\n",
    "                book_data = parse_book_page(link, session)\n",
    "            except Exception as e:\n",
    "                print(f\"  ! Failed parse {link}: {e}\")\n",
    "                continue\n",
    "\n",
    "            results.append(book_data)\n",
    "            print(f\"  + scraped: {book_data['title'][:60]}\")\n",
    "\n",
    "            # stop early if reached target\n",
    "            if len(results) >= max_books:\n",
    "                break\n",
    "\n",
    "        # find next listing page\n",
    "        next_page = find_next_page(soup, next_page)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    # Reorder columns to match requested variables (plus product_page_url)\n",
    "    cols = [\n",
    "        \"category\", \"code\", \"cover\", \"title\", \"rating\",\n",
    "        \"price (excl. tax)\", \"price (incl. tax)\", \"tax\",\n",
    "        \"stock status\", \"number of stock available\", \"description\",\n",
    "        \"number of reviews\", \"product_page_url\"\n",
    "    ]\n",
    "    # keep only columns that exist\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    # target 1000 books (the site has 1000 books)\n",
    "    print(\"Starting scrape of books.toscrape.com ...\")\n",
    "    df = scrape_all_books(max_books=1000)\n",
    "\n",
    "    print(f\"Scraped {len(df)} books. Saving to CSV...\")\n",
    "    df.to_csv(\"books.csv\", index=False)\n",
    "    try:\n",
    "        df.to_parquet(\"books.parquet\", index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"Saved to books.csv (and books.parquet if pyarrow installed).\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    # show small sample\n",
    "    print(df.head(3).to_dict(orient=\"records\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79405afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('books.csv')\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
